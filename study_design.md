Investigating How LLMs Represent False Beliefs

Background: LLMs and Theory of Mind (ToM)

Recent research suggests that large language models can handle false-belief tasks (a classic test of Theory of Mind). For example, Kosinski (2024) evaluated 11 LLMs on 40 diverse false-belief scenarios (with matched true-belief controls) and found a sharp scaling of performance ￼. Smaller pre-2022 models failed all ToM tasks, while GPT-3.5 (ChatGPT March 2023) solved about 20% and GPT-4 (June 2023) solved 75% of tasks, matching the success rate of ~6-year-old children ￼. This striking result raised the possibility that a rudimentary ToM capability may have emerged spontaneously in advanced LLMs as a byproduct of predictive language training ￼. However, it also spurred debate over whether the models are truly reasoning about others’ beliefs or merely pattern-matching on familiar story structures ￼. Critics noted LLMs might “recognize” false-belief scenarios from training data and recall memorized answers, rather than internally representing beliefs per se ￼. In response, researchers crafted rigorously controlled tasks (including reversals and true-belief controls) to ensure that simple heuristics or shallow cues would fail ￼. Under these strict conditions, GPT-4 still performed strongly (30/40 tasks) and even exhibited dynamic reasoning – for instance, initially assuming a character knew something, then revising its inference upon learning the character hadn’t observed the key event ￼. Such behavior suggests the model may be genuinely tracking the character’s knowledge state, not just applying a canned solution.

Open-source models: The above findings mostly involved closed models like GPT-4. To design follow-up analyses, we should identify powerful open-source LLMs that can be tested and instrumented locally. The most capable open models currently include Meta’s LLaMA-2 (especially the 70B parameter version) and other large Transformers like Falcon 40B or MPT-30B, which approach the performance of GPT-3.5 on many tasks. While they may not reach GPT-4’s ToM prowess, they likely possess some ToM ability. For instance, even a 70M-parameter model (Pythia-70M) can encode others’ beliefs in its representations to a limited degree ￼. With scaling, open models should achieve at least partial success on false-belief tests – e.g. an instruct-tuned LLaMA-2 70B might solve basic tasks similarly to GPT-3.5. First, we should empirically validate this by giving our chosen open models a battery of false-belief scenarios (potentially using Kosinski’s dataset or similar prompts). We would check that the model answers correctly when, say, Alice mistakenly believes an object is still in its original location, versus control scenarios where her belief is true. This behavioral sanity check ensures the model has non-trivial ToM-like behavior to analyze. If performance is low, we might boost it via few-shot prompting or fine-tuning on simple belief reasoning examples (taking care not to leak test answers). Recent studies show that prompt techniques can substantially improve ToM performance in LLMs ￼, so we could apply such strategies to get the open model reliably passing some tasks.

With a capable open model in hand, we can proceed to inspect its internal workings during ToM tasks. Below we survey recent literature on visualizing and analyzing the internal mechanisms of LLMs, then propose specific analyses to uncover how false beliefs might be represented in neural activations or attention patterns.

Analyzing Internal Representations of Beliefs in LLMs

Researchers have begun to develop interpretability techniques to peek inside transformer models and see how they might represent concepts like “who knows what.” Notably, several very recent works provide evidence that LLMs form internal representations of different agents’ beliefs, rather than just producing correct answers in a black-box way. We can draw on these techniques as inspiration for our follow-up analyses. Key approaches include examining attention head patterns, probing hidden activation spaces for encodings of belief, identifying individual “ToM neurons”, and performing causal interventions on the model’s internal state. Each method can shed light on whether and how the model internally distinguishes false belief contexts. Below, we detail each approach and suggest concrete tests for our open-source model.

1. Attention Pattern Analysis: Tracking Perspectives

One straightforward avenue is to analyze the model’s attention masks/weights to see if it allocates separate focus to each character’s perspective. Transformers have multiple attention heads that attend to different parts of the input; some of these heads might specifically track what a given character knows or has observed. For example, a 2025 study by Li et al. constructed a multimodal ToM benchmark (“GridToM”) and found that certain attention heads distinguish different observers’ viewpoints in a scenario ￼ ￼. Their analysis showed that specific heads had distinct attention patterns depending on whose perspective was currently relevant, indicating the model was separating each agent’s knowledge internally ￼. In our text-only case, we can similarly look for heads that focus on information visible to the protagonist vs. information only the reader/narrator knows.

How to test: For a given false-belief story, we can visualize the attention matrices for the final layers when the model is answering the question. Do we see, for instance, an attention head that strongly attends from the question (e.g. “Where will Alice look for the object?”) back to the last place Alice saw the object in the story, rather than the object’s actual current location? That would be a hint that the head is tracking Alice’s belief. We could systematically measure attention weights: e.g., define two sets of tokens – those related to Alice’s last knowledge vs. those revealing the actual state – and see if any head consistently attends more to the former in false-belief scenarios. By comparing with true-belief controls (where Alice did witness the move), we can check if the attention shifts appropriately. If in a true-belief scenario the same head instead attends to the actual current location, that’s strong evidence the head’s role is to follow the protagonist’s knowledge. In the literature, certain attention heads in GPT-like models are known to track coreference and identity over long text (so-called “induction heads” that recall earlier mentions of a name). Those could be repurposed to maintain continuity of each character’s viewpoint. We may discover a similar pattern here: heads that, when the story says “Alice leaves the room,” subsequently focus on Alice and ignore events happening behind her back. Visualizing these attention distributions (for example, as heatmaps over the story text) can make such patterns apparent.

Expected outcome: If the model truly encodes false beliefs distinctly, we anticipate finding at least one attention head (likely in a mid-to-upper transformer layer) whose weight distribution differentiates knowledge vs. ignorance. For instance, when predicting Alice’s action, that head might attend heavily to a sentence like “Alice saw the toy in the box initially” and very little to “Bob later moved the toy,” effectively limiting itself to Alice’s last known information. Such a finding would mirror the multimodal ToM result that attention heads carry perspective-specific information ￼. We can further validate the importance of these heads by ablating or boosting them. If we zero-out the suspected “belief-tracking” head, does the model often fail false-belief questions (e.g., answering with the actual location instead)? Conversely, if we boost that head’s output (as Li et al. did by nudging attention along certain directions), does the model become more consistent in tracking beliefs ￼? A 2025 study demonstrated that a simple intervention – amplifying the activity of heads most sensitive to perspective differences – significantly improved a model’s ToM question accuracy ￼. We can attempt a similar intervention in our model: identify top heads by some metric (see next section on probing), then amplify their focus on the protagonist’s perspective during inference to see if answers improve. This would confirm those heads are functionally encoding the false-belief information.

2. Probing Hidden States for Belief Representations

Another powerful approach is to use latent probing techniques to detect whether the model’s hidden activations explicitly encode the belief states of characters. Recent literature shows this is feasible: linear probes can often read out high-level concepts from LLM activations ￼. In fact, two independent works in 2024 (Zhu et al. and Bortoletto et al.) applied probing specifically to Theory-of-Mind contexts. They found that it’s possible to train a simple linear classifier on the internal embeddings of an LLM to predict what a story character believes, and even whether that belief is true or false ￼. In other words, at some layer, the model’s vector representation of the context is rich enough to linearly separate “Alice thinks X is in location A” vs “Alice thinks X is in location B.” Moreover, steering the model via these probes can change its answers: if the probe detects an incorrect latent belief, one can adjust the representation and often get the model to fix its answer ￼. This suggests the model’s knowledge of others’ beliefs isn’t just implicit in the output – it’s an actual feature in the neural state that we can locate and manipulate.

How to test: We can perform a probing analysis on our model’s hidden states during false-belief tasks. Concretely, take a set of narratives where a character either has a true or false belief about some fact (e.g. location of an object). After the model reads the story (but before it generates the answer), extract the residual stream or final-layer embedding of the context. Label these representations with the ground-truth status of the character’s belief (or with the content of the belief, if we want a finer classification). Then train a logistic regression (or even just examine the mean differences) to see if the belief state is encoded. If the probe can predict, say, “character believes the object is in the box vs. in the drawer” with accuracy far above chance, that means the model has internally differentiated those scenarios ￼. Prior work found exactly this: even relatively small models like 70M parameters had latent dimensions that correspond to the omniscient correct belief about a scenario ￼. Larger models and ones fine-tuned for following instructions showed even clearer linear separability of belief information ￼. We should probe multiple layers to see where in the network this information becomes salient. Perhaps early layers won’t encode the belief yet (since the model first reads the text literally), but somewhere in the middle or later layers, the divergent storylines (protagonist saw vs. didn’t see the critical event) will lead to diverging internal states. If we find, for example, that at layer 24 the probe accuracy jumps above 90%, that indicates the model by that layer has formed a distinct representation of the protagonist’s belief.

An advanced variation is to probe attention head outputs specifically. Li et al. (2025) did this for a vision-language model: they took the output vectors of each attention head and trained probes to classify true vs false belief contexts ￼. This allowed them to pinpoint which heads carry the belief-related signal. They observed that many heads (spread from middle to final layers) achieved high probe accuracy, whereas early-layer heads did not ￼. We can do the same in our model: iterate over heads and see which ones’ activations make good predictors of the character’s belief state. We might find, for instance, that layer 20, head 4 encodes “Alice thinks X is true/false” clearly, while others do not. Such identification would complement the qualitative attention analysis above with a quantitative measure of each head’s relevance.

Importantly, probing not only tests for existence of a representation but also opens the door to controlled interventions. If we can identify a direction in hidden state space that corresponds to “the protagonist believes X,” we can try injecting or altering that to test causally. For example, if the model answers a question incorrectly (mixing up the character’s belief), we could nudge its hidden state along the direction the probe deems as “false belief” to see if the answer corrects. Zhu et al. reported that steering the internal representation via a probe’s findings significantly affected model outputs on belief questions ￼. In practice, we could use the linear classifier’s weights as a direction vector: add a small amount in the “belief = true” or “belief = false” direction and see if the answer flips accordingly. A successful flip would be strong evidence that the dimension we found indeed corresponds to the concept of interest (the character’s belief content). This kind of test moves us from pure correlation to causation – it shows that manipulating the purported belief representation has a predictable effect on behavior.

In summary, linear probe tests will help verify that our model explicitly represents false beliefs in its activations. We expect to find at least one layer (likely later in the network) where a linear readout can tell what the agent knows versus what is actually true ￼. If not, then the model might be solving tasks in a more superficial way. But given prior findings (belief representations in various LLMs, even small ones ￼), we anticipate positive results here.

3. Neuron-Level Analysis: ToM Neurons

Going even more fine-grained, we can search for individual neurons (features in a layer’s activation vector) that correlate with theory-of-mind computations. In neural network interpretability, researchers often look for single neurons that activate for specific concepts. A recent study by Jamali et al. (2023) did exactly this for ToM: they found that in large language models, certain neurons in the deeper layers respond strongly to scenarios requiring false-belief reasoning, analogous to how specific neurons in the human brain fire during ToM tasks ￼. These “ToM neurons” in the model showed activity patterns predictive of whether the model would succeed or fail at a belief question ￼. This is a fascinating parallel to neuroscience (e.g. mirror neurons or cells in the primate brain associated with belief reasoning).

How to test: Using our open model, we can perform a systematic search for neurons related to false-belief processing. One approach is correlation analysis: run the model on a battery of stories and record the activation of each neuron (each hidden unit in each layer) just before answering the question. We label each instance as needing false-belief reasoning (e.g. the protagonist’s belief diverges from reality) or a control scenario (protagonist knows the truth). Then we compute which neurons have the largest difference in average activation between false-belief vs true-belief conditions. High-variance neurons might be candidates that specifically respond to the presence of others’ ignorance. We could also check neurons’ correlation with output correctness: e.g., find neurons whose activation is high exactly when the model correctly answers a false-belief question. Jamali et al. reported that specific units in later layers tracked ToM performance (when those neurons were strongly activated, the model tended to get the answer right) ￼.

Once we identify a set of candidate neurons, we can probe their behavior further. For instance, we can inspect which input phrases strongly activate those neurons (by searching for prompts that maximize that neuron’s activation). Perhaps we find one neuron that spikes whenever the text includes a phrase like “didn’t realize that…”, “unbeknownst to [the character]…”, or any cue of hidden information. Such semantic tuning would suggest the neuron is picking up on when a character is unaware of something. Another neuron might activate when the question asks about someone’s belief or perspective (“Where will X look for…?”). By reading the top contexts for each neuron, we essentially interpret its role in the model’s logic.

To further validate, we can do neuron ablation experiments. If a neuron truly carries critical ToM information, zeroing it out (or otherwise intervening) might impair the model’s performance on false-belief questions. We could test the model on some tasks with the neuron activated vs. deactivated and see if answers change. Conversely, stimulating the neuron (setting it to a high value) might systematically bias the model’s answers (possibly towards assuming a false belief or vice versa). For example, activating a neuron that usually indicates “the character is ignorant” might cause the model to answer as if the character has a false belief even in a scenario where they don’t – which would be a telling sign of that neuron’s function. We should of course interpret results carefully, as networks are redundant and no single neuron might be solely responsible; but significant performance drops upon ablation would indicate that neuron was part of an important internal circuit for belief reasoning.

Finding such neurons would deepen our understanding of where in the network ToM reasoning happens. Are the “belief neurons” concentrated in a specific layer (e.g. layer 40 of a 70-layer model)? Do multiple neurons share the load (likely yes)? It also lets us draw interesting parallels to biology (e.g., are there analogs to human temporoparietal junction activity for belief reasoning?). Given prior evidence that single units can align with high-level concepts including mental states ￼, we are optimistic about identifying at least a few meaningful neurons.

4. Causal Tracing and Mechanistic Explanations

Beyond identifying correlates of belief (heads, vectors, neurons), we want to explain the mechanism: how does the model propagate and use the information about who knows what? For this, we can use causal tracing or intervention methods inspired by recent mechanistic interpretability research. The idea is to pinpoint where in the computation the critical information (e.g. “Alice does not know the ball moved”) is stored and how it influences the answer.

One technique is activation patching (causal interchange). We take two related inputs: one false-belief scenario and one true-belief scenario (for example, one story where Alice leaves the room before the toy is moved, and one where Alice stays and sees the move). We then run the model on both, and at a chosen layer, we swap the internal activations between the two runs, then let the forward pass continue. By doing this swap at different layers, we can ask: at what layer does the difference between these scenarios become “locked in” such that swapping after that point flips the model’s answer? If swapping at layer 30 causes the model that originally was answering “the box” to now answer “the drawer” (the other scenario’s answer), then layers ≤30 have not yet irreversibly committed to an answer, whereas by layer 31 the representations diverged in a crucial way. This helps map the timeline of information processing: perhaps the model reads the story, and only when it reads the final question does it integrate everything and encode the needed belief state around layer N. We might discover that right after reading the story (before the question), the activations are similar between the false/true variants, but when the question “Where will Alice look?” is processed, suddenly a divergence appears in layer 25+ corresponding to whether Alice knows the latest state. This would show how the model conditionally applies knowledge based on context at a specific stage.

Another approach is causal scrubbing: systematically ablate or randomize parts of the input or intermediate computation to see what is necessary for the correct reasoning. For instance, if we remove the sentence describing the protagonist’s absence and the model now fails the task (giving a wrong answer), that confirms the model was using that information (unsurprising). More interestingly, if we zero out the residual stream components along certain principal directions (like those identified by the probe as encoding belief), we can test if those directions are indeed carrying the needed info. If zeroing a “belief vector” causes the model to become confused or answer as if the character’s knowledge state was lost, that’s strong evidence for a causal role of that component.

We can also leverage the linear representation findings discussed earlier in a causal way. The survey by Nguyen (2025) notes that transformers often represent latent variables linearly in the residual stream ￼. In other words, there might literally be a subspace or linear combination of neurons corresponding to “the true state of the world” and another to “the agent’s perceived state.” If we can identify those (perhaps via PCA or the probe weights), we could attempt an intervention: e.g., replace the part of the residual stream encoding the agent’s belief with the part encoding the actual state, and see if the model’s answer then treats the agent as if they were omniscient. This would be analogous to feeding the model counterfactual knowledge. Conversely, replacing the “world-state” representation with the agent’s limited view might cause the model to answer as if it only knew what the agent knows. By such counterfactual manipulations of internal state, we directly test whether the model has disentangled representations of “reality vs beliefs.” If the model didn’t have any such separation, these swaps would either do nothing (if no distinct belief encoding exists) or just scramble the output unintelligibly. But if the model does maintain a structured internal model of others’ beliefs, then swapping those should produce coherent, if altered, outputs (like answering with the wrong location intentionally when we inject a false belief internally).

All these mechanistic tests will help us map the causal flow: from reading the story, to encoding who knows what, to using that to select an answer. Ideally, we might end up with a circuit-level explanation such as: “Layer 15’s attention heads copy the object’s location into a separate ‘belief track’ for Alice; layer 25’s MLP neurons compare Alice’s believed location with the actual location and mark a discrepancy; finally, layer 30’s attention uses that to choose the answer token corresponding to Alice’s belief.” While this level of detail might be ambitious, even partial insights (like identifying a pivotal layer or head) are valuable. They move us beyond just observing what the model can do to understanding how it implements a form of ToM.

5. Visualization and Clustering of Belief Representations

For a more intuitive understanding, we can also visualize the model’s internal representations of belief. Using dimensionality reduction (PCA, t-SNE, or UMAP) on hidden states, we can check if scenarios cluster by the protagonist’s belief. For example, take high-dimensional vectors from the model (could be the final-layer CLS token embedding, or an average of certain attention heads) for many stories: do all the false-belief scenario vectors cluster apart from the true-belief ones? If yes, that’s a visual confirmation that the model’s latent space differentiates these situations. We might even see multiple clusters corresponding to what the specific belief is (e.g. “believes location = X” vs “= Y”). If using PCA, perhaps the first principal component of the residual stream is aligned with “truth vs falsehood of protagonist’s belief.” In prior work, attention head activations were projected and indeed showed separable distributions for true vs false belief cases ￼. Li et al. visualized the attention feature space of a head and found that data points clustered into distinct groups corresponding to correct perspective tracking vs not, with clear decision boundaries between true and false beliefs ￼. This kind of plot, while requiring some care to interpret, can be a compelling demonstration of the model’s internal geometry of belief states.

Additionally, we could present a trajectory view: as the model reads through a story step by step, track the evolving hidden state in a lower-dimensional projection. When a critical event occurs (like the object being moved without Alice seeing), we might see the trajectory in state-space diverge from a trajectory of an otherwise identical story where Alice did see it. Such a divergence point would pinpoint exactly where in the narrative the model’s internal belief representation for Alice shifts. This complements the earlier causal tracing by providing a more continuous, visual perspective. If, for instance, after the “Alice leaves the room” sentence the trajectory veers into a distinct region of state-space (signaling the model now treats Alice as ignorant), that’s evidence of a discrete internal state change corresponding to the story’s false-belief condition.

Summary of Proposed Tests and Analyses

In light of the above literature and ideas, here is a summary of follow-up analyses we recommend to capture how LLMs represent false beliefs internally:
	•	Behavioral Verification: First, ensure the chosen open-source LLM (e.g. LLaMA-2 70B or similar) can solve a set of false-belief tasks. Test it on classic ToM scenarios and matched controls, verifying it answers correctly only when appropriate (mirroring prior results that powerful models can pass many false-belief tests ￼).
	•	Attention Head Examination: Visualize and quantify the model’s attention patterns for evidence of perspective-taking. Identify any attention heads that focus on information available to the protagonist versus hidden information. Check if these heads behave differently in false-belief vs true-belief stories (e.g. attending to the wrong location in false-belief cases). Ablate or boost such heads to test their influence on answers.
	•	Hidden State Probing: Use linear probes on intermediate and final layer activations to detect encoded belief states. Train simple classifiers to predict the protagonist’s (mis)belief from the model’s hidden state ￼. Determine which layer and even which attention heads carry the highest signal for “what does character X believe?”. If a probe finds a clear “belief vector,” perform causal interventions by manipulating that vector and observing changes in the model’s answer ￼.
	•	Neuron Identification: Search for individual neurons strongly correlated with false-belief reasoning ￼. For each neuron, evaluate its activation across many stories to see if it selectively fires for key conditions (e.g. a character being unaware). Test the effect of toggling these neurons on model performance. This can reveal “ToM neurons” analogous to those observed in human brain studies ￼.
	•	Causal Tracing of Information Flow: Perform activation patching between true- and false-belief story variants to localize where the model differentiates the protagonist’s belief. Identify the layer (or even attention head/MLP) at which swapping representations flips the outcome. Use this to map the hierarchy of processing (which components update or store the belief state). Additionally, try directly swapping or editing the putative belief representations in the residual stream to see if the model’s answer follows the implanted belief (demonstrating a causal role for those representations).
	•	Representation Visualization: Project high-dimensional activation states into 2D/3D to visualize clustering by belief context. Show that the model’s internal states form distinct clusters for different belief conditions (true vs false, or different belief contents) ￼. Plot the trajectory of state change as the story unfolds to pinpoint where the model ‘realizes’ a character holds a false belief.

By conducting these analyses, we leverage the latest interpretability research to open the “black box” of the language model’s mind. Prior work has already given us hints – from attention heads that separate viewpoints ￼, to linear decoders that read out beliefs ￼, to neurons echoing ToM-related activity ￼ – that LLMs may encode false beliefs in discernible ways. Our plan applies these insights to state-of-the-art open models. This not only helps validate whether those models truly understand false beliefs or are faking it, but also contributes to the broader goal of “Neuroscience of LLMs.” By treating the model as an artificial brain to probe and experiment on, we can chart how advanced language models form beliefs about others – a step toward more transparent and trustworthy AI systems.